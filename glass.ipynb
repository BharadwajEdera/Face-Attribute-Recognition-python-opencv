{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import imageio as im\n",
    "from PIL import Image\n",
    "import os\n",
    "import dlib\n",
    "from imutils.face_utils.helpers import rect_to_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mglass\u001b[0m/  \u001b[01;34mglasses\u001b[0m/  glass.ipynb  labels.npy  \u001b[01;34mw_glass\u001b[0m/  \u001b[01;34mwithout_glasses\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cropping the faces from images\n",
    "\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# dest = 'glass/'\n",
    "# img_dir = ['glasses/']\n",
    "# for im_dir in img_dir:\n",
    "#     images = os.listdir(im_dir)\n",
    "#     for i,image in enumerate(images):\n",
    "#         img = cv2.imread(im_dir + image)\n",
    "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         rects = detector(gray, 1) \n",
    "#         for j,rect in enumerate(rects):\n",
    "#             (x, y, w, h) = rect_to_bb(rect)\n",
    "# #             cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,255),1)\n",
    "#             sub_face = gray[y:y+h, x:x+w]\n",
    "#             cv2.imwrite(dest+str(i)+'_'+str(j)+'.jpg',sub_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "labels = []\n",
    "img_dir = ['glass/','w_glass/']\n",
    "for im_dir in img_dir:\n",
    "    images = os.listdir(im_dir)\n",
    "    for image in images:\n",
    "        img = cv2.imread(im_dir + image)\n",
    "        img = cv2.resize(img, (36, 36)) # need to resize the image into common size\n",
    "        all_images.append(img)\n",
    "        labels.append(img_dir.index(im_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 75, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('glass/101_0.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(all_images)\n",
    "data = data.astype('float32')\n",
    "data /= 255.0\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(open('glass.npy', 'wb'), data)\n",
    "# np.save(open('labels.npy', 'wb'), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compindia/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(592, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labels = np.asarray(labels)\n",
    "labels = labels.reshape(-1, 1)\n",
    "ohe_y=OneHotEncoder()\n",
    "Y=ohe_y.fit_transform(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = shuffle(data,Y, random_state=42)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(36, 36, 3...)`\n",
      "  \n",
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3,activation='relu',input_shape=(36, 36, 3)))\n",
    "\n",
    "classifier.add(Convolution2D(64,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(128,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(256,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(256,activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(2,activation='softmax'))\n",
    "\n",
    "classifier.compile(Adam(lr=0.0001, decay=1e-6),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 119 samples\n",
      "Epoch 1/25\n",
      "473/473 [==============================] - 13s 27ms/step - loss: 0.6988 - acc: 0.5011 - val_loss: 0.6704 - val_acc: 0.5126\n",
      "Epoch 2/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.5737 - acc: 0.7273 - val_loss: 0.4474 - val_acc: 0.7731\n",
      "Epoch 3/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.3791 - acc: 0.8520 - val_loss: 0.3157 - val_acc: 0.8655\n",
      "Epoch 4/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.3286 - acc: 0.8647 - val_loss: 0.2757 - val_acc: 0.8824\n",
      "Epoch 5/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.2948 - acc: 0.8837 - val_loss: 0.2512 - val_acc: 0.8992\n",
      "Epoch 6/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.2245 - acc: 0.9154 - val_loss: 0.2257 - val_acc: 0.9076\n",
      "Epoch 7/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.2230 - acc: 0.9070 - val_loss: 0.2497 - val_acc: 0.8824\n",
      "Epoch 8/25\n",
      "473/473 [==============================] - 13s 28ms/step - loss: 0.2000 - acc: 0.9218 - val_loss: 0.1838 - val_acc: 0.9244\n",
      "Epoch 9/25\n",
      "473/473 [==============================] - 13s 28ms/step - loss: 0.1778 - acc: 0.9408 - val_loss: 0.1565 - val_acc: 0.9580\n",
      "Epoch 10/25\n",
      "473/473 [==============================] - 13s 27ms/step - loss: 0.1820 - acc: 0.9493 - val_loss: 0.1538 - val_acc: 0.9496\n",
      "Epoch 11/25\n",
      "473/473 [==============================] - 12s 25ms/step - loss: 0.1540 - acc: 0.9429 - val_loss: 0.1527 - val_acc: 0.9412\n",
      "Epoch 12/25\n",
      "473/473 [==============================] - 13s 27ms/step - loss: 0.1367 - acc: 0.9429 - val_loss: 0.1428 - val_acc: 0.9496\n",
      "Epoch 13/25\n",
      "473/473 [==============================] - 13s 28ms/step - loss: 0.1086 - acc: 0.9598 - val_loss: 0.1443 - val_acc: 0.9328\n",
      "Epoch 14/25\n",
      "473/473 [==============================] - 14s 29ms/step - loss: 0.0964 - acc: 0.9662 - val_loss: 0.1179 - val_acc: 0.9496\n",
      "Epoch 15/25\n",
      "473/473 [==============================] - 14s 29ms/step - loss: 0.0954 - acc: 0.9683 - val_loss: 0.1171 - val_acc: 0.9496\n",
      "Epoch 16/25\n",
      "473/473 [==============================] - 14s 29ms/step - loss: 0.0694 - acc: 0.9789 - val_loss: 0.1282 - val_acc: 0.9580\n",
      "Epoch 17/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0686 - acc: 0.9789 - val_loss: 0.1392 - val_acc: 0.9496\n",
      "Epoch 18/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0522 - acc: 0.9852 - val_loss: 0.1417 - val_acc: 0.9496\n",
      "Epoch 19/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0331 - acc: 0.9915 - val_loss: 0.1182 - val_acc: 0.9580\n",
      "Epoch 20/25\n",
      "473/473 [==============================] - 14s 31ms/step - loss: 0.0393 - acc: 0.9852 - val_loss: 0.1962 - val_acc: 0.9412\n",
      "Epoch 21/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0474 - acc: 0.9852 - val_loss: 0.1887 - val_acc: 0.9412\n",
      "Epoch 22/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0344 - acc: 0.9915 - val_loss: 0.1398 - val_acc: 0.9496\n",
      "Epoch 23/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0247 - acc: 0.9873 - val_loss: 0.1798 - val_acc: 0.9328\n",
      "Epoch 24/25\n",
      "473/473 [==============================] - 14s 30ms/step - loss: 0.0354 - acc: 0.9873 - val_loss: 0.1363 - val_acc: 0.9496\n",
      "Epoch 25/25\n",
      "473/473 [==============================] - 13s 28ms/step - loss: 0.0161 - acc: 0.9958 - val_loss: 0.1740 - val_acc: 0.9496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7c90a5eb8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=1, shuffle=True, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 34, 34, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 651,330\n",
      "Trainable params: 651,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "labels = []\n",
    "img_dir = ['glass/','w_glass/']\n",
    "for im_dir in img_dir:\n",
    "    images = os.listdir(im_dir)\n",
    "    for image in images:\n",
    "        img = cv2.imread(im_dir + image)\n",
    "        img = cv2.resize(img, (48, 48)) # need to resize the image into common size\n",
    "        all_images.append(img)\n",
    "        labels.append(img_dir.index(im_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(all_images)\n",
    "data = data.astype('float32')\n",
    "data /= 255.0\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compindia/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(592, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.save(open('glass.npy', 'wb'), data)\n",
    "# np.save(open('labels.npy', 'wb'), y_train)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labels = np.asarray(labels)\n",
    "labels = labels.reshape(-1, 1)\n",
    "ohe_y=OneHotEncoder()\n",
    "Y=ohe_y.fit_transform(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(48, 48, 3...)`\n",
      "  \n",
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  app.launch_new_instance()\n",
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "/home/compindia/.local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 119 samples\n",
      "Epoch 1/25\n",
      "473/473 [==============================] - 25s 52ms/step - loss: 0.6941 - acc: 0.5455 - val_loss: 0.6039 - val_acc: 0.8067\n",
      "Epoch 2/25\n",
      "473/473 [==============================] - 26s 55ms/step - loss: 0.4532 - acc: 0.7907 - val_loss: 0.3476 - val_acc: 0.8571\n",
      "Epoch 3/25\n",
      "473/473 [==============================] - 23s 49ms/step - loss: 0.3223 - acc: 0.8795 - val_loss: 0.2634 - val_acc: 0.8992\n",
      "Epoch 4/25\n",
      "473/473 [==============================] - 24s 51ms/step - loss: 0.2488 - acc: 0.9091 - val_loss: 0.2190 - val_acc: 0.9244\n",
      "Epoch 5/25\n",
      "473/473 [==============================] - 22s 47ms/step - loss: 0.2520 - acc: 0.9049 - val_loss: 0.2641 - val_acc: 0.8908\n",
      "Epoch 6/25\n",
      "473/473 [==============================] - 22s 47ms/step - loss: 0.2158 - acc: 0.9175 - val_loss: 0.2008 - val_acc: 0.9244\n",
      "Epoch 7/25\n",
      "473/473 [==============================] - 22s 46ms/step - loss: 0.2156 - acc: 0.9197 - val_loss: 0.2001 - val_acc: 0.9076\n",
      "Epoch 8/25\n",
      "473/473 [==============================] - 22s 47ms/step - loss: 0.1760 - acc: 0.9408 - val_loss: 0.2011 - val_acc: 0.9328\n",
      "Epoch 9/25\n",
      "473/473 [==============================] - 22s 46ms/step - loss: 0.1631 - acc: 0.9366 - val_loss: 0.1841 - val_acc: 0.9328\n",
      "Epoch 10/25\n",
      "473/473 [==============================] - 23s 48ms/step - loss: 0.1509 - acc: 0.9493 - val_loss: 0.1368 - val_acc: 0.9160\n",
      "Epoch 11/25\n",
      "473/473 [==============================] - 22s 46ms/step - loss: 0.1361 - acc: 0.9514 - val_loss: 0.1629 - val_acc: 0.9328\n",
      "Epoch 12/25\n",
      "473/473 [==============================] - 21s 45ms/step - loss: 0.1203 - acc: 0.9641 - val_loss: 0.1196 - val_acc: 0.9496\n",
      "Epoch 13/25\n",
      "473/473 [==============================] - 22s 46ms/step - loss: 0.1292 - acc: 0.9556 - val_loss: 0.1248 - val_acc: 0.9328\n",
      "Epoch 14/25\n",
      "473/473 [==============================] - 22s 46ms/step - loss: 0.0912 - acc: 0.9704 - val_loss: 0.1295 - val_acc: 0.9580\n",
      "Epoch 15/25\n",
      "473/473 [==============================] - 21s 45ms/step - loss: 0.0808 - acc: 0.9725 - val_loss: 0.6084 - val_acc: 0.7647\n",
      "Epoch 16/25\n",
      "473/473 [==============================] - 24s 51ms/step - loss: 0.0906 - acc: 0.9746 - val_loss: 0.0876 - val_acc: 0.9412\n",
      "Epoch 17/25\n",
      "473/473 [==============================] - 25s 53ms/step - loss: 0.0584 - acc: 0.9852 - val_loss: 0.0830 - val_acc: 0.9496\n",
      "Epoch 18/25\n",
      "473/473 [==============================] - 29s 61ms/step - loss: 0.0514 - acc: 0.9873 - val_loss: 0.0813 - val_acc: 0.9496\n",
      "Epoch 19/25\n",
      "473/473 [==============================] - 28s 60ms/step - loss: 0.0357 - acc: 0.9915 - val_loss: 0.0934 - val_acc: 0.9580\n",
      "Epoch 20/25\n",
      "473/473 [==============================] - 29s 62ms/step - loss: 0.0479 - acc: 0.9873 - val_loss: 0.0774 - val_acc: 0.9580\n",
      "Epoch 21/25\n",
      "473/473 [==============================] - 26s 56ms/step - loss: 0.0317 - acc: 0.9958 - val_loss: 0.1292 - val_acc: 0.9496\n",
      "Epoch 22/25\n",
      "473/473 [==============================] - 25s 54ms/step - loss: 0.0327 - acc: 0.9915 - val_loss: 0.1121 - val_acc: 0.9412\n",
      "Epoch 23/25\n",
      "473/473 [==============================] - 28s 59ms/step - loss: 0.0157 - acc: 0.9958 - val_loss: 0.1100 - val_acc: 0.9496\n",
      "Epoch 24/25\n",
      "473/473 [==============================] - 30s 63ms/step - loss: 0.0318 - acc: 0.9873 - val_loss: 0.0995 - val_acc: 0.9580\n",
      "Epoch 25/25\n",
      "473/473 [==============================] - 31s 66ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0888 - val_acc: 0.9496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7642927f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = shuffle(data,Y, random_state=42)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3,activation='relu',input_shape=(48, 48, 3)))\n",
    "\n",
    "classifier.add(Convolution2D(64,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(128,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(256,3,3,activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(256,activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(2,activation='softmax'))\n",
    "\n",
    "classifier.compile(Adam(lr=0.0001, decay=1e-6),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=1, shuffle=True, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(184, 64) (339, 219)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(469, 222) (655, 407)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(15, 242) (201, 428)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(43, 142) (266, 365)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(464, 93) (687, 316)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(96, 82) (225, 211)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(442, 81) (597, 236)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(27, 241) (348, 562)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(332, 204) (718, 590)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(110, 110) (664, 665)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(27, 134) (348, 455)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(332, 119) (718, 504)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(439, 142) (662, 365)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(92, 118) (315, 341)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1.0702001e-20 1.0000000e+00]\n",
      "[(386, 57) (572, 242)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(97, 57) (283, 242)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(442, 305) (597, 460)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(139, 268) (268, 397)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(439, 167) (662, 390)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(27, 241) (348, 562)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n",
      "[(348, 241) (669, 562)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(106, 66) (195, 156)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[1. 0.]\n",
      "[(414, 93) (637, 316)]\n",
      "(48, 48)\n",
      "(1, 48, 48, 3)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "src = 'test/'\n",
    "for images in os.listdir('test/'):\n",
    "    image = cv2.imread(src+images)\n",
    "    image = imutils.resize(image, width=700)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 1)   # detect faces in the grayscale image\n",
    "    for i,rect in enumerate(rects):\n",
    "        print(rect)\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(255,255,255),1)\n",
    "        cv2.putText(image,\"FACE #{}\".format(i+1),(x-10,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "        x = max(0,rect.left())\n",
    "        y = max(0,rect.top())\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, (48, 48))\n",
    "        cv2.imwrite('test/test.jpg',face)\n",
    "        face1 = cv2.imread('test/test.jpg')\n",
    "        face = face1.reshape((1,48, 48, 3))\n",
    "        result = classifier.predict(face)[0]\n",
    "        res = 'NO' if result[1] > result[0] else 'YES'\n",
    "#         print(result)\n",
    "        cv2.putText(image,\"Glasses :{}\".format(res),(x+10,y+10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "        cv2.imshow('image',image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('glass_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# target_size = 224\n",
    "# from keras.applications.mobilenetv2 import MobileNetV2\n",
    "# from keras.layers import Dense, Input, Dropout\n",
    "# from keras.models import Model\n",
    "\n",
    "# def build_model( ):\n",
    "#     input_tensor = Input(shape=(target_size, target_size, 3))\n",
    "#     base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
    "#         input_shape=(target_size, target_size, 3), pooling='avg')\n",
    "\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = True  # trainable has to be false in order to freeze the layers\n",
    "#     op = Dense(1, activation = 'softmax')(base_model.output)\n",
    "\n",
    "#     output_tensor = Dense(1, activation='softmax')(op)\n",
    "\n",
    "#     model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# model = build_model()\n",
    "# model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs = 5, verbose = 1,batch_size=1 ,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# renaming the images in to the new directory\n",
    "\n",
    "source ='images/'          \n",
    "dest = 'men/'\n",
    "images = sorted(os.listdir(source))\n",
    "k=603\n",
    "for image in images:\n",
    "    print(image)\n",
    "    img  = cv2.imread(os.path.join(source,image))\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.waitKey(1000)\n",
    "    img = cv2.resize(img, (224, 224)) # need to resize the image into common size\n",
    "    extension = image.split('.')[-1]\n",
    "    k+=1\n",
    "    cv2.imwrite(dest+str(k)+'.'+extension,img)\n",
    "\n",
    "len(images)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
